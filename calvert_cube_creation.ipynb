{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209d7e86",
   "metadata": {},
   "source": [
    "### 930am Monday June 16th: Taking the mostly working section plots and making the new calvert cubes using this modified interpolation technique ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f4989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.dates as mdates\n",
    "import cmocean as cm\n",
    "import waypoint_distance as wd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_section(ds, topo, xlim=(77,0)):\n",
    "    \"\"\"\n",
    "    Plot temperature section for a given file\n",
    "\n",
    "    Parameters:\n",
    "    - ds: xarray.Dataset, a .nc file for each transect\n",
    "    - xlim: float, maximum distance (km) along transect to plot\n",
    "    \"\"\"\n",
    "\n",
    "    # ─── Styling ─────────────────────────────────────────\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 12,\n",
    "        'axes.titlesize': 20,\n",
    "        'axes.labelsize': 20,\n",
    "        'xtick.labelsize': 15,\n",
    "        'ytick.labelsize': 15,\n",
    "        'legend.fontsize': 20,\n",
    "        'figure.titlesize': 20})\n",
    "\n",
    "    # ─── Extract variables ──────────────────────────────\n",
    "    along = ds['along'].values\n",
    "    depth = ds['depth'].values\n",
    "    temperature = ds['temperature'].values\n",
    "    pdens = ds['potential_density'].values - 1000  # sigma-theta\n",
    "    lon = ds['longitude'].values\n",
    "    lat = ds['latitude'].values\n",
    "    time_top = ds['time'].values  # shape (along,)\n",
    "\n",
    "    # ─── Interpolate Bathymetry ─────────────────────────\n",
    "    interp_bathy = topo['Band1'].interp(\n",
    "        lon=xr.DataArray(lon, dims='along'),\n",
    "        lat=xr.DataArray(lat, dims='along'),\n",
    "        method='nearest')\n",
    "    ocean_floor = -interp_bathy.values\n",
    "    depth_grid, along_grid = np.meshgrid(depth, along, indexing='ij')  # shape: (depth, along)\n",
    "\n",
    "    # Expand bottom_depths to match depth grid shape\n",
    "    bathymetry_floor = np.tile(ocean_floor, (len(depth), 1))\n",
    "\n",
    "    # Create mask where depth > bottom\n",
    "    mask = depth_grid > bathymetry_floor\n",
    "\n",
    "    # ------- Plotting ------- #\n",
    "    fig, ax = plt.subplots(figsize=(1.5 * 1.5 * 6.4, 1.5 * 4.8))\n",
    "    \n",
    "    # Fill below bathymetry to 420 m\n",
    "    ax.fill_between(along / 1000, ocean_floor, 420,\n",
    "                    where=~np.isnan(ocean_floor),\n",
    "                    facecolor='grey', zorder=1)\n",
    "    \n",
    "    # Plot temperature\n",
    "    cf = ax.pcolormesh(along / 1000, depth, temperature, \n",
    "                    shading='auto', cmap=cm.cm.thermal, \n",
    "                    vmin=5.3, vmax=10, zorder=2)\n",
    "\n",
    "    # Plot bathymetry\n",
    "    ax.plot(along / 1000, ocean_floor, color='black', linewidth=2)\n",
    "\n",
    "    # Isopycnals\n",
    "    for levels, color, lw in [\n",
    "        (np.linspace(24, 27, 7), 'black', 0.5),\n",
    "        # ([26.6], 'white', 2),\n",
    "        ([26.7], 'lime', 2),\n",
    "        ([26.8], 'red', 2),\n",
    "        ([26.9], 'blue', 2)]:\n",
    "        cf_iso = ax.contour(along / 1000, depth, pdens, levels=levels,\n",
    "                            colors=color, linewidths=lw, linestyles='-')\n",
    "        if lw != 0.3:\n",
    "            ax.clabel(cf_iso, fmt='%1.2f')\n",
    "\n",
    "    # ─── Top Axis with Time Labels ──────────────────────\n",
    "    along_km = along / 1000\n",
    "    nticks = 8\n",
    "    idx_ticks = np.linspace(0, len(along_km) - 1, nticks, dtype=int)\n",
    "    tick_locs = along_km[idx_ticks]\n",
    "    tick_times = time_top[idx_ticks]\n",
    "\n",
    "    # Remove NaT\n",
    "    valid_mask = ~pd.isna(tick_times)\n",
    "    tick_locs = tick_locs[valid_mask]\n",
    "    tick_times = tick_times[valid_mask]\n",
    "    tick_labels = [pd.to_datetime(t).strftime('%b %d %H:%M') for t in tick_times]\n",
    "\n",
    "    ax_top = ax.secondary_xaxis('top')\n",
    "    ax_top.set_xticks(tick_locs)\n",
    "    ax_top.set_xticklabels(tick_labels, rotation=30, ha='center', fontsize=10)\n",
    "\n",
    "    # ─── Labels and Limits ──────────────────────────────\n",
    "    ax.set_xlabel('Along-Transect Distance (km)')\n",
    "    ax.set_ylabel('Depth (m)')\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_ylim(420, 0)\n",
    "    ax.set_xlim(xlim)\n",
    "\n",
    "    earliest_time = pd.to_datetime(min(tick_times))\n",
    "    tstr = earliest_time.strftime('%Y-%m-%d')\n",
    "    ax.set_title(f'Temperature Section ({tstr})')\n",
    "\n",
    "    plt.colorbar(cf, ax=ax, label='Temperature (°C)')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def interpolate(ds, step=50, extrapolate=True):\n",
    "    import numpy as np\n",
    "    import xarray as xr\n",
    "\n",
    "    # Remove duplicate along values\n",
    "    _, index_unique = np.unique(ds['along'], return_index=True)\n",
    "    ds = ds.isel(time=index_unique)\n",
    "\n",
    "    # Build regular along grid\n",
    "    min_along = np.floor(ds['along'].min().item() / step) * step\n",
    "    max_along = np.ceil(ds['along'].max().item() / step) * step\n",
    "    along_grid = np.arange(min_along, max_along + 1, step)\n",
    "\n",
    "    # Swap time with along for interpolation\n",
    "    ds = ds.swap_dims({'time': 'along'})\n",
    "    ds_interp = ds.interp(along=along_grid)\n",
    "\n",
    "    # Interpolate time manually (since it's not numeric by default)\n",
    "    interp_time = np.interp(\n",
    "        along_grid,\n",
    "        ds['along'].values,\n",
    "        ds['time'].values.astype('datetime64[ns]').astype('float64'),\n",
    "        left=np.nan,\n",
    "        right=np.nan\n",
    "    )\n",
    "    ds_interp['time'] = ('along', interp_time.astype('datetime64[ns]'))\n",
    "\n",
    "    # Get along mask for 0–20 km region\n",
    "    mask_along = (ds_interp['along'] >= 0) & (ds_interp['along'] <= 20000)\n",
    "\n",
    "    # Subset temperature in that range\n",
    "    temp_sub = ds_interp['temperature'].where(mask_along, drop=True)\n",
    "\n",
    "    # Find the deepest depth where at least one valid (non-NaN) temperature exists\n",
    "    valid_depths = ds_interp['depth'][~np.all(np.isnan(temp_sub), axis=1)]\n",
    "    max_valid_depth = valid_depths.max().item()\n",
    "\n",
    "    # Limit to valid depth range\n",
    "    ds_interp = ds_interp.sel(depth=ds_interp['depth'] <= max_valid_depth)\n",
    "\n",
    "    # ─── Vectorized valid_temp_depth assignment ─────\n",
    "    temp = ds_interp['temperature'].values  # (depth, along)\n",
    "    depth_vals = ds_interp['depth'].values\n",
    "    valid_mask = ~np.isnan(temp)\n",
    "    reversed_mask = valid_mask[::-1, :]\n",
    "    first_valid_idx_from_bottom = reversed_mask.argmax(axis=0)\n",
    "    has_valid_data = valid_mask.any(axis=0)\n",
    "    valid_depths = np.where(has_valid_data,\n",
    "                            depth_vals[-1 - first_valid_idx_from_bottom],\n",
    "                            np.nan)\n",
    "    ds_interp['valid_temp_depth'] = ('along', valid_depths)\n",
    "\n",
    "    # ─── Fill NaNs along each depth row using nearest ─────\n",
    "    for var in ds_interp.data_vars:\n",
    "        da = ds_interp[var]\n",
    "        if 'along' not in da.dims or 'depth' not in da.dims:\n",
    "            continue\n",
    "\n",
    "        filled_rows = []\n",
    "        depths = ds_interp['depth'].values\n",
    "        along_vals = ds_interp['along'].values\n",
    "\n",
    "        for i in range(len(depths)):\n",
    "            row = da.isel(depth=i)\n",
    "            filled = row.interpolate_na(\n",
    "                dim='along',\n",
    "                method='nearest',\n",
    "                fill_value='extrapolate')\n",
    "            filled_rows.append(filled.values)\n",
    "\n",
    "        # Rebuild variable\n",
    "        new_da = xr.DataArray(\n",
    "            data=np.array(filled_rows),\n",
    "            dims=('depth', 'along'),\n",
    "            coords={'depth': depths, 'along': along_vals})\n",
    "        ds_interp[var] = new_da\n",
    "\n",
    "    return ds_interp\n",
    "\n",
    "def clean_and_interpolate(file_pathway, topo):\n",
    "\n",
    "    ds = xr.open_dataset(file_pathway)\n",
    "\n",
    "    waypoint_lon = np.array([-127.950, -128.115, -128.243, -128.514, -128.646, -128.798])\n",
    "    waypoint_lat = np.array([51.757, 51.705, 51.715, 51.450, 51.4165, 51.408])\n",
    "    central_lat = 51.715\n",
    "\n",
    "    alongx, acrossx, _ = wd.get_simple_distance(\n",
    "        shiplon=ds['longitude'].values,\n",
    "        shiplat=ds['latitude'].values,\n",
    "        wplon=waypoint_lon,\n",
    "        wplat=waypoint_lat,\n",
    "        central_lat=central_lat)\n",
    "\n",
    "    ds = ds.assign(along=('time', alongx), across=('time', acrossx))\n",
    "\n",
    "    peak_idx = int(np.argmax(ds['along'].values))\n",
    "    ds_out = ds.isel(time=slice(0, peak_idx + 1))\n",
    "    ds_return = ds.isel(time=slice(peak_idx + 1, None))\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for leg, name in [(ds_out, \"out\"), (ds_return, \"return\")]:\n",
    "        try:\n",
    "            if leg['time'].size < 2 or np.count_nonzero(~np.isnan(leg['along'])) < 10:\n",
    "                continue\n",
    "\n",
    "            prev_len = -1\n",
    "            while prev_len != len(leg['time']):\n",
    "                if len(leg['along']) < 3:\n",
    "                    raise ValueError(\"Too short for gradient\")\n",
    "                prev_len = len(leg['time'])\n",
    "                grad = np.gradient(leg['along'].values)\n",
    "                keep_mask = grad > 0 if name == \"out\" else grad < 0\n",
    "                leg = leg.sel(time=keep_mask)\n",
    "\n",
    "            if np.count_nonzero(~np.isnan(leg['along'])) < 100:\n",
    "                continue\n",
    "\n",
    "            # Interpolate before assigning to results\n",
    "            leg_interp = interpolate(leg, step=50)\n",
    "            leg_interp = mask_dataset(leg_interp, topo)\n",
    "            results[name] = leg_interp\n",
    "            # results[name] = leg\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {name} leg in {file_pathway}: {e}\")\n",
    "            continue\n",
    "        \n",
    "    # Access outbound leg\n",
    "    ds_out_cleaned = results.get(\"out\")\n",
    "\n",
    "    # Access return leg\n",
    "    ds_return_cleaned = results.get(\"return\")\n",
    "\n",
    "    return ds_out_cleaned, ds_return_cleaned\n",
    "\n",
    "def mask_dataset(ds, topo):\n",
    "    \"\"\"\n",
    "    Applies depth-based masking to all 2D (depth, along) variables using valid_temp_depth\n",
    "    and bathymetry clearance logic.\n",
    "\n",
    "    Parameters:\n",
    "    - ds: xarray.Dataset, must include 'valid_temp_depth'\n",
    "    - topo: xarray.Dataset, bathymetry\n",
    "\n",
    "    Returns:\n",
    "    - ds_masked: xarray.Dataset with masked versions of all (depth, along) variables\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract variables\n",
    "    along = ds['along'].values\n",
    "    depth = ds['depth'].values\n",
    "    lon = ds['longitude'].values\n",
    "    lat = ds['latitude'].values\n",
    "\n",
    "    if 'valid_temp_depth' not in ds:\n",
    "        raise ValueError(\"Dataset must include 'valid_temp_depth'\")\n",
    "\n",
    "    # Interpolate bathymetry\n",
    "    ocean_floor = -topo['Band1'].interp(\n",
    "        lon=xr.DataArray(lon, dims='along'),\n",
    "        lat=xr.DataArray(lat, dims='along'),\n",
    "        method='nearest').values\n",
    "\n",
    "    valid_depths = ds['valid_temp_depth'].values\n",
    "    local_clearance = ocean_floor - valid_depths\n",
    "\n",
    "    # Average clearance in trusted region\n",
    "    trusted = (along >= 20000) & (along <= 77000)\n",
    "    mean_clearance = np.nanmean(local_clearance[trusted])\n",
    "\n",
    "    # Compute adaptive mask depth\n",
    "    mask_depth = np.minimum(\n",
    "        np.where(local_clearance <= mean_clearance,\n",
    "                 ocean_floor - local_clearance,\n",
    "                 ocean_floor - mean_clearance),\n",
    "        ocean_floor\n",
    "    )\n",
    "\n",
    "    # Apply mask to all 2D (depth, along) variables\n",
    "    for var in ds.data_vars:\n",
    "        da = ds[var]\n",
    "        if set(da.dims) == {'depth', 'along'}:\n",
    "            arr = da.values.copy()\n",
    "            for j in range(len(along)):\n",
    "                limit_depth = mask_depth[j]\n",
    "                if np.isnan(limit_depth):\n",
    "                    arr[:, j] = np.nan\n",
    "                else:\n",
    "                    arr[depth > limit_depth, j] = np.nan\n",
    "            # Save masked version\n",
    "            ds[var] = (('depth', 'along'), arr)\n",
    "\n",
    "    return ds\n",
    "\n",
    "#############################################\n",
    "## _____________ Processing ______________ ##\n",
    "#############################################\n",
    "if False:\n",
    "\n",
    "    def process_and_plot(file_pathway):\n",
    "\n",
    "        topo = xr.open_dataset(os.path.expanduser('~/Desktop/british_columbia_3_msl_2013.nc'))\n",
    "\n",
    "        ds_out_cleaned, ds_return_cleaned = clean_and_interpolate(file_pathway, topo)\n",
    "        if ds_out_cleaned is not None:\n",
    "            plot_section(ds_out_cleaned, topo)\n",
    "        else:\n",
    "            print(\"No outbound leg found.\")\n",
    "\n",
    "        if ds_return_cleaned is not None:\n",
    "            plot_section(ds_return_cleaned, topo)\n",
    "        else:\n",
    "            print(\"No return leg found.\")\n",
    "        return ds_return_cleaned\n",
    "\n",
    "    file_pathway = '~/CalvertLine_reprocessed/dfo-hal1002-20240702_grid_delayed.nc'\n",
    "    ds_return_cleaned = process_and_plot(file_pathway)\n",
    "if False: \n",
    "    # Load bathymetry once\n",
    "    topo = xr.open_dataset(os.path.expanduser('~/Desktop/british_columbia_3_msl_2013.nc'))\n",
    "\n",
    "    input_dir = Path(\"~/Users/martinwilliamson/Desktop/dfo-hal1002-20250506_grid.nc\").expanduser()\n",
    "    output_dir = Path(\"~/Desktop/cleaned_transects\").expanduser()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_files = sorted(input_dir.glob(\"*_grid.nc\"))\n",
    "\n",
    "    for file_path in all_files:\n",
    "        print(f\"🧼 Cleaning & interpolating: {file_path.name}\")\n",
    "\n",
    "        try:\n",
    "            ds_out, ds_return = clean_and_interpolate(str(file_path), topo)\n",
    "\n",
    "            for ds, leg in [(ds_out, \"out\"), (ds_return, \"return\")]:\n",
    "                if ds is None:\n",
    "                    print(f\"⚠️ No {leg} leg found in {file_path.name}\")\n",
    "                    continue\n",
    "\n",
    "                valid_times = ds['time'].values[~np.isnan(ds['time'].values)]\n",
    "                if len(valid_times) == 0:\n",
    "                    print(f\"⚠️ Skipping {file_path.name} — no valid times in {leg} leg.\")\n",
    "                    continue\n",
    "\n",
    "                timestamp = pd.to_datetime(valid_times[0]).strftime('%Y%m%d')\n",
    "                out_path = output_dir / f\"{timestamp}_{leg}.nc\"\n",
    "                ds.to_netcdf(out_path)\n",
    "                print(f\"✅ Saved {leg}: {out_path.name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed on {file_path.name}: {e}\")\n",
    "        \n",
    "file_path = Path(\"~/Desktop/dfo-hal1002-20220804_grid_delayed.nc\").expanduser()\n",
    "topo = xr.open_dataset(os.path.expanduser('~/Desktop/Summer 2025 Python/british_columbia_3_msl_2013.nc'))\n",
    "output_dir = Path(\"~/Desktop/Summer 2025 Python/cleaned_transects\").expanduser()\n",
    "ds_out, ds_return = clean_and_interpolate(str(file_path), topo)\n",
    "\n",
    "# Then optionally save it like before:\n",
    "for ds, leg in [(ds_out, \"out\"), (ds_return, \"return\")]:\n",
    "    if ds is None:\n",
    "        print(f\"⚠️ No {leg} leg found in {file_path.name}\")\n",
    "        continue\n",
    "\n",
    "    valid_times = ds['time'].values[~np.isnan(ds['time'].values)]\n",
    "    if len(valid_times) == 0:\n",
    "        print(f\"⚠️ Skipping {file_path.name} — no valid times in {leg} leg.\")\n",
    "        continue\n",
    "\n",
    "    timestamp = pd.to_datetime(valid_times[0]).strftime('%Y%m%d')\n",
    "    out_path = output_dir / f\"{timestamp}_{leg}.nc\"\n",
    "    ds.to_netcdf(out_path)\n",
    "    print(f\"✅ Saved {leg}: {out_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f1a7a",
   "metadata": {},
   "source": [
    "# Adding new glider files to the cube without remaking full cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ecfb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "existing_cube = xr.open_dataset(\"~/Desktop/Summer 2025 Python/calvert_cube.nc\")\n",
    "\n",
    "# Explicit paths to the 2 new files\n",
    "new_files = [\n",
    "    Path(\"~/Desktop/Summer 2025 Python/cleaned_transects/20220804_out.nc\").expanduser(),\n",
    "    # Path(\"~/Desktop/Summer 2025 Python/cleaned_transects/20250625_return.nc\").expanduser()\n",
    "]\n",
    "\n",
    "new_datasets = []\n",
    "for nc_file in new_files:\n",
    "    ds = xr.open_dataset(nc_file)\n",
    "\n",
    "    transect_label = nc_file.stem\n",
    "    ds = ds.expand_dims(transect=[transect_label])\n",
    "    new_datasets.append(ds)\n",
    "    print(f\"✅ Prepared: {transect_label}\")\n",
    "\n",
    "# Always demote longitude and latitude to variables in ALL datasets\n",
    "\n",
    "def demote_coords(ds):\n",
    "    # If the coord is present as a coordinate, reset it\n",
    "    for coord in ['longitude', 'latitude']:\n",
    "        if coord in ds.coords:\n",
    "            ds = ds.reset_coords(coord)\n",
    "    return ds\n",
    "\n",
    "# Demote in existing cube\n",
    "existing_cube = demote_coords(existing_cube)\n",
    "\n",
    "# Demote in new datasets\n",
    "new_datasets_cleaned = []\n",
    "for ds in new_datasets:\n",
    "    ds = demote_coords(ds)\n",
    "    new_datasets_cleaned.append(ds)\n",
    "\n",
    "updated_cube = xr.concat(\n",
    "    [existing_cube] + new_datasets_cleaned,\n",
    "    dim=\"transect\",\n",
    "    compat=\"no_conflicts\"\n",
    ")\n",
    "\n",
    "updated_cube.to_netcdf(\"~/Desktop/Summer 2025 Python/transect_cube.nc\")\n",
    "print(\"✅ Saved updated cube to ~/Desktop/Summer 2025 Python/transect_cube.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ee1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "existing_cube = xr.open_dataset(\"~/Desktop/Summer 2025 Python/calvert_cube.nc\")\n",
    "\n",
    "# Explicit paths to the 2 new files\n",
    "new_files = [\n",
    "    Path(\"~/Desktop/Summer 2025 Python/cleaned_transects/20220804_out.nc\").expanduser(),\n",
    "    # Path(\"~/Desktop/Summer 2025 Python/cleaned_transects/20250625_return.nc\").expanduser()\n",
    "]\n",
    "\n",
    "new_datasets = []\n",
    "for nc_file in new_files:\n",
    "    ds = xr.open_dataset(nc_file)\n",
    "\n",
    "    transect_label = nc_file.stem\n",
    "    ds = ds.expand_dims(transect=[transect_label])\n",
    "    new_datasets.append(ds)\n",
    "    print(f\"✅ Prepared: {transect_label}\")\n",
    "\n",
    "# Always demote longitude and latitude to variables in ALL datasets\n",
    "\n",
    "def demote_coords(ds):\n",
    "    # If the coord is present as a coordinate, reset it\n",
    "    for coord in ['longitude', 'latitude']:\n",
    "        if coord in ds.coords:\n",
    "            ds = ds.reset_coords(coord)\n",
    "    return ds\n",
    "\n",
    "# Demote in existing cube\n",
    "existing_cube = demote_coords(existing_cube)\n",
    "\n",
    "# Demote in new datasets\n",
    "new_datasets_cleaned = []\n",
    "for ds in new_datasets:\n",
    "    ds = demote_coords(ds)\n",
    "    new_datasets_cleaned.append(ds)\n",
    "\n",
    "updated_cube = xr.concat(\n",
    "    [existing_cube] + new_datasets_cleaned,\n",
    "    dim=\"transect\",\n",
    "    compat=\"no_conflicts\")\n",
    "\n",
    "updated_cube.to_netcdf(\"~/Desktop/Summer 2025 Python/calvert_cube.nc\")\n",
    "print(\"✅ Saved updated cube to ~/Desktop/Summer 2025 Python/calvert_cube.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "input_dir = Path(\"~/Desktop/cleaned_transects\").expanduser()\n",
    "all_files = sorted(input_dir.glob(\"*.nc\"))\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for nc_file in all_files:\n",
    "    try:\n",
    "        ds = xr.open_dataset(nc_file)\n",
    "\n",
    "        # Promote lon/lat to regular variables if they are coordinates\n",
    "        for coord in ['longitude', 'latitude']:\n",
    "            if coord in ds.coords:\n",
    "                ds = ds.reset_coords(coord)\n",
    "\n",
    "        # Add transect label\n",
    "        transect_label = nc_file.stem\n",
    "        ds = ds.expand_dims(transect=[transect_label])\n",
    "        datasets.append(ds)\n",
    "\n",
    "        print(f\"✅ Added: {transect_label}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping {nc_file.name}: {e}\")\n",
    "\n",
    "# Combine all into one cube\n",
    "if datasets:\n",
    "    cube = xr.concat(datasets, dim='transect')\n",
    "    cube.to_netcdf(\"~/Desktop/transect_cube.nc\")\n",
    "    print(\"✅ Saved cube to ~/Desktop/transect_cube.nc\")\n",
    "else:\n",
    "    print(\"⚠️ No valid datasets to combine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421030aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load bathymetry once\n",
    "topo = xr.open_dataset(os.path.expanduser('~/Desktop/british_columbia_3_msl_2013.nc'))\n",
    "\n",
    "input_dir = Path(\"~/Desktop/cleaned_transects\").expanduser()\n",
    "\n",
    "# Loop through all cleaned transect files\n",
    "for nc_file in sorted(input_dir.glob(\"*.nc\")):\n",
    "    try:\n",
    "        ds = xr.open_dataset(nc_file)\n",
    "        print(f\"📈 Plotting: {nc_file.name}\")\n",
    "\n",
    "        plot_section(ds, topo)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to plot {nc_file.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638397ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.dates as mdates\n",
    "import cmocean as cm\n",
    "import waypoint_distance as wd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "def plot_oxygen_section(ds, topo, xlim=(77, 0)):\n",
    "    \"\"\"\n",
    "    Plot oxygen concentration section for a given transect dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - ds: xarray.Dataset for the transect\n",
    "    - topo: xarray.Dataset with bathymetry (e.g., from GEBCO or similar)\n",
    "    - xlim: tuple of (max, min) along-track km for x-axis limits\n",
    "    \"\"\"\n",
    "\n",
    "    # ─── Styling ─────────────────────────────────────────\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 12,\n",
    "        'axes.titlesize': 20,\n",
    "        'axes.labelsize': 20,\n",
    "        'xtick.labelsize': 15,\n",
    "        'ytick.labelsize': 15,\n",
    "        'legend.fontsize': 20,\n",
    "        'figure.titlesize': 20})\n",
    "\n",
    "    # ─── Extract variables ──────────────────────────────\n",
    "    along = ds['along'].values\n",
    "    depth = ds['depth'].values\n",
    "    oxygen = ds['oxygen_concentration'].values\n",
    "    pdens = ds['potential_density'].values - 1000\n",
    "    lon = ds['longitude'].values\n",
    "    lat = ds['latitude'].values\n",
    "    time_top = ds['time'].values\n",
    "\n",
    "    # ─── Interpolate Bathymetry ─────────────────────────\n",
    "    interp_bathy = topo['Band1'].interp(\n",
    "        lon=xr.DataArray(lon, dims='along'),\n",
    "        lat=xr.DataArray(lat, dims='along'),\n",
    "        method='nearest')\n",
    "    ocean_floor = -interp_bathy.values\n",
    "    depth_grid, along_grid = np.meshgrid(depth, along, indexing='ij')\n",
    "    bathymetry_floor = np.tile(ocean_floor, (len(depth), 1))\n",
    "    mask = depth_grid > bathymetry_floor\n",
    "\n",
    "    # ─── Plotting ───────────────────────────────────────\n",
    "    fig, ax = plt.subplots(figsize=(1.5 * 1.5 * 6.4, 1.5 * 4.8))\n",
    "\n",
    "    # Fill below bathymetry to 420m\n",
    "    ax.fill_between(along / 1000, ocean_floor, 420,\n",
    "                    where=~np.isnan(ocean_floor),\n",
    "                    facecolor='grey', zorder=1)\n",
    "\n",
    "    # Oxygen colormap\n",
    "    cf = ax.pcolormesh(along / 1000, depth, oxygen,\n",
    "                       shading='auto', cmap='inferno',\n",
    "                       vmin=0, vmax=200, zorder=2)\n",
    "\n",
    "    # Bathymetry line\n",
    "    ax.plot(along / 1000, ocean_floor, color='black', linewidth=2)\n",
    "\n",
    "    # Isopycnals\n",
    "    for levels, color, lw in [\n",
    "        (np.linspace(24, 27, 7), 'black', 0.5),\n",
    "        ([26.7], 'lime', 2),\n",
    "        ([26.8], 'red', 2),\n",
    "        ([26.9], 'blue', 2)]:\n",
    "        iso = ax.contour(along / 1000, depth, pdens, levels=levels,\n",
    "                         colors=color, linewidths=lw, linestyles='-')\n",
    "        if lw > 0.5:\n",
    "            ax.clabel(iso, fmt='%1.2f')\n",
    "\n",
    "    # ─── Top Axis with Time ─────────────────────────────\n",
    "    along_km = along / 1000\n",
    "    nticks = 8\n",
    "    idx_ticks = np.linspace(0, len(along_km) - 1, nticks, dtype=int)\n",
    "    tick_locs = along_km[idx_ticks]\n",
    "    tick_times = time_top[idx_ticks]\n",
    "\n",
    "    valid_mask = ~pd.isna(tick_times)\n",
    "    tick_locs = tick_locs[valid_mask]\n",
    "    tick_times = tick_times[valid_mask]\n",
    "    tick_labels = [pd.to_datetime(t).strftime('%b %d %H:%M') for t in tick_times]\n",
    "\n",
    "    if len(tick_locs) > 0:\n",
    "        ax_top = ax.secondary_xaxis('top')\n",
    "        ax_top.set_xticks(tick_locs)\n",
    "        ax_top.set_xticklabels(tick_labels, rotation=30, ha='center', fontsize=10)\n",
    "\n",
    "    # ─── Labels & Limits ────────────────────────────────\n",
    "    ax.set_xlabel('Along-Transect Distance (km)')\n",
    "    ax.set_ylabel('Depth (m)')\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_ylim(420, 0)\n",
    "    ax.set_xlim(xlim)\n",
    "\n",
    "    # Title with timestamp\n",
    "    if len(tick_times) > 0:\n",
    "        tstr = pd.to_datetime(tick_times[0]).strftime('%Y-%m-%d')\n",
    "        ax.set_title(f'Oxygen Section ({tstr})')\n",
    "    else:\n",
    "        ax.set_title('Oxygen Section')\n",
    "\n",
    "    # Colorbar\n",
    "    plt.colorbar(cf, ax=ax, label='Oxygen (μmol/kg)')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Load bathymetry once\n",
    "topo = xr.open_dataset(os.path.expanduser('~/Desktop/british_columbia_3_msl_2013.nc'))\n",
    "\n",
    "input_dir = Path(\"~/Desktop/cleaned_transects\").expanduser()\n",
    "\n",
    "# Loop through all cleaned transect files\n",
    "for nc_file in sorted(input_dir.glob(\"*.nc\")):\n",
    "    try:\n",
    "        ds = xr.open_dataset(nc_file)\n",
    "        print(f\"📈 Plotting: {nc_file.name}\")\n",
    "\n",
    "        plot_oxygen_section(ds, topo)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to plot {nc_file.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c3bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pathway = '~/Desktop/dfo-hal1002-20250506_grid_delayed.nc'\n",
    "\n",
    "if True:\n",
    "\n",
    "    def process_and_plot(file_pathway):\n",
    "\n",
    "        topo = xr.open_dataset(os.path.expanduser('~/Desktop/british_columbia_3_msl_2013.nc'))\n",
    "\n",
    "        ds_out_cleaned, ds_return_cleaned = clean_and_interpolate(file_pathway, topo)\n",
    "        if ds_out_cleaned is not None:\n",
    "            plot_section(ds_out_cleaned, topo)\n",
    "        else:\n",
    "            print(\"No outbound leg found.\")\n",
    "\n",
    "        if ds_return_cleaned is not None:\n",
    "            plot_section(ds_return_cleaned, topo)\n",
    "        else:\n",
    "            print(\"No return leg found.\")\n",
    "        return ds_return_cleaned\n",
    "    ds_return_cleaned = process_and_plot(file_pathway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0b3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing cube creation\n",
    "\n",
    "# import xarray as xr\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import waypoint_distance as wd\n",
    "\n",
    "# def clean_mission(file_pathway):\n",
    "#     ds = xr.open_dataset(file_pathway)\n",
    "\n",
    "#     # Project onto along/across\n",
    "#     waypoint_lon = np.array([-127.950, -128.115, -128.243, -128.514, -128.646, -128.798])\n",
    "#     waypoint_lat = np.array([51.757, 51.705, 51.715, 51.450, 51.4165, 51.408])\n",
    "#     central_lat = 51.715\n",
    "\n",
    "#     alongx, acrossx, _ = wd.get_simple_distance(\n",
    "#         shiplon=ds['longitude'].values,\n",
    "#         shiplat=ds['latitude'].values,\n",
    "#         wplon=waypoint_lon,\n",
    "#         wplat=waypoint_lat,\n",
    "#         central_lat=central_lat)\n",
    "\n",
    "#     ds = ds.assign(along=('time', alongx), across=('time', acrossx))\n",
    "\n",
    "#     peak_idx = int(np.argmax(ds['along'].values))\n",
    "#     ds_out = ds.isel(time=slice(0, peak_idx + 1))\n",
    "#     ds_return = ds.isel(time=slice(peak_idx + 1, None))\n",
    "\n",
    "#     for leg, name in [(ds_out, \"out\"), (ds_return, \"return\")]:\n",
    "#         if leg['time'].size < 2 or np.count_nonzero(~np.isnan(leg['along'])) < 10:\n",
    "#             print(f\"Skipping {name} leg: not enough points\")\n",
    "#             if name == \"out\": ds_out = None\n",
    "#             else: ds_return = None\n",
    "#             continue\n",
    "\n",
    "#         prev_len = -1\n",
    "#         while prev_len != len(leg['time']):\n",
    "#             prev_len = len(leg['time'])\n",
    "#             grad = np.gradient(leg['along'])\n",
    "#             keep_mask = grad > 0 if name == \"out\" else grad < 0\n",
    "#             leg = leg.sel(time=keep_mask)\n",
    "\n",
    "#         if np.count_nonzero(~np.isnan(leg['along'])) < 100:\n",
    "#             print(f\"Dropping {name} leg: too few valid points\")\n",
    "#             if name == \"out\": ds_out = None\n",
    "#             else: ds_return = None\n",
    "#             continue\n",
    "\n",
    "#         if name == \"out\": ds_out = interpolate(leg.set_coords('along'))\n",
    "#         else: ds_return = interpolate(leg.set_coords('along'))\n",
    "\n",
    "#     if ds_out is not None: ds_out = ds_out.assign_coords(mission_type='Outbound')\n",
    "#     if ds_return is not None: ds_return = ds_return.assign_coords(mission_type='Return')\n",
    "\n",
    "#     return ds_out, ds_return\n",
    "\n",
    "# def interpolate(ds):\n",
    "#     _, index_unique = np.unique(ds['along'], return_index=True)\n",
    "#     ds = ds.isel(time=index_unique)\n",
    "\n",
    "#     min_along = np.floor(ds['along'].min().item() / 50) * 50\n",
    "#     max_along = np.ceil(ds['along'].max().item() / 50) * 50\n",
    "#     along_grid = np.arange(min_along, max_along + 1, 50)\n",
    "\n",
    "#     ds = ds.swap_dims({'time': 'along'})\n",
    "#     ds_interp = ds.interp(along=along_grid)\n",
    "\n",
    "#     interp_time = np.interp(\n",
    "#         along_grid,\n",
    "#         ds['along'].values,\n",
    "#         ds['time'].values.astype('datetime64[ns]').astype('float64'),\n",
    "#         left=np.nan,\n",
    "#         right=np.nan\n",
    "#     )\n",
    "#     ds_interp['time'] = ('along', interp_time.astype('datetime64[ns]'))\n",
    "\n",
    "#     return ds_interp\n",
    "\n",
    "# def append_to_calvert_cube(ds_transect, cube_path='~/Desktop/calvert_cube.nc', source_file=None):\n",
    "#     cube_path = os.path.expanduser(cube_path)\n",
    "\n",
    "#     if 'transect' not in ds_transect.dims:\n",
    "#         ds_transect = ds_transect.expand_dims(transect=[0])\n",
    "\n",
    "#     if 'transect_time' not in ds_transect.coords:\n",
    "#         valid_times = pd.to_datetime(ds_transect['time'].values, errors='coerce').dropna()\n",
    "#         transect_time = np.datetime64(valid_times[0]) if len(valid_times) > 0 else np.datetime64('NaT')\n",
    "#         ds_transect = ds_transect.assign_coords(transect_time=('transect', [transect_time]))\n",
    "\n",
    "#     if source_file:\n",
    "#         ds_transect.attrs['source_file'] = os.path.basename(source_file)\n",
    "\n",
    "#     if not os.path.exists(cube_path):\n",
    "#         ds_transect.to_netcdf(cube_path)\n",
    "#         print(f\"🟢 Created new cube: {cube_path}\")\n",
    "#         return\n",
    "\n",
    "#     cube = xr.open_dataset(cube_path)\n",
    "#     cube.load(); cube.close()\n",
    "\n",
    "#     processed_files = cube.attrs.get('source_file', [])\n",
    "#     if isinstance(processed_files, str):\n",
    "#         processed_files = [processed_files]\n",
    "#     if os.path.basename(source_file) in processed_files:\n",
    "#         print(f\"⚠️ Skipping {source_file}: already in cube.\")\n",
    "#         return\n",
    "\n",
    "#     new_index = cube.sizes['transect']\n",
    "#     ds_transect = ds_transect.assign_coords(transect=[new_index])\n",
    "#     ds_combined = xr.concat([cube, ds_transect], dim='transect')\n",
    "#     ds_combined = ds_combined.sortby('transect_time')\n",
    "\n",
    "#     ds_combined.attrs['source_file'] = processed_files + [os.path.basename(source_file)]\n",
    "#     ds_combined.to_netcdf(cube_path, mode='w')\n",
    "#     print(f\"✅ Appended to cube: {source_file}\")\n",
    "\n",
    "# def add_file_to_cube(filepath):\n",
    "#     ds_out, ds_return = clean_mission(filepath)\n",
    "#     if ds_out is not None:\n",
    "#         append_to_calvert_cube(ds_out, source_file=filepath + \"_out\")\n",
    "#     else:\n",
    "#         print(f\"Skipped outbound for {filepath}\")\n",
    "#     if ds_return is not None:\n",
    "#         append_to_calvert_cube(ds_return, source_file=filepath + \"_return\")\n",
    "#     else:\n",
    "#         print(f\"Skipped return for {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca86fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TRYING AGAIN\" \n",
    "# import xarray as xr\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "# import waypoint_distance as wd\n",
    "\n",
    "# def interpolate(ds, step=50):\n",
    "#     _, index_unique = np.unique(ds['along'], return_index=True)\n",
    "#     ds = ds.isel(time=index_unique)\n",
    "\n",
    "#     min_along = np.floor(ds['along'].min().item() / step) * step\n",
    "#     max_along = np.ceil(ds['along'].max().item() / step) * step\n",
    "#     along_grid = np.arange(min_along, max_along + 1, step)\n",
    "\n",
    "#     ds = ds.swap_dims({'time': 'along'})\n",
    "#     ds_interp = ds.interp(along=along_grid)\n",
    "\n",
    "#     interp_time = np.interp(\n",
    "#         along_grid,\n",
    "#         ds['along'].values,\n",
    "#         ds['time'].values.astype('datetime64[ns]').astype('float64'),\n",
    "#         left=np.nan,\n",
    "#         right=np.nan\n",
    "#     )\n",
    "#     ds_interp['time'] = ('along', interp_time.astype('datetime64[ns]'))\n",
    "\n",
    "#     return ds_interp\n",
    "\n",
    "# def clean_and_interpolate(file_pathway):\n",
    "#     ds = xr.open_dataset(file_pathway)\n",
    "\n",
    "#     waypoint_lon = np.array([-127.950, -128.115, -128.243, -128.514, -128.646, -128.798])\n",
    "#     waypoint_lat = np.array([51.757, 51.705, 51.715, 51.450, 51.4165, 51.408])\n",
    "#     central_lat = 51.715\n",
    "\n",
    "#     alongx, acrossx, _ = wd.get_simple_distance(\n",
    "#         shiplon=ds['longitude'].values,\n",
    "#         shiplat=ds['latitude'].values,\n",
    "#         wplon=waypoint_lon,\n",
    "#         wplat=waypoint_lat,\n",
    "#         central_lat=central_lat)\n",
    "\n",
    "#     ds = ds.assign(along=('time', alongx), across=('time', acrossx))\n",
    "\n",
    "#     peak_idx = int(np.argmax(ds['along'].values))\n",
    "#     ds_out = ds.isel(time=slice(0, peak_idx + 1))\n",
    "#     ds_return = ds.isel(time=slice(peak_idx + 1, None))\n",
    "\n",
    "#     results = {}\n",
    "#     for leg, name in [(ds_out, \"out\"), (ds_return, \"return\")]:\n",
    "#         if leg['time'].size < 2 or np.count_nonzero(~np.isnan(leg['along'])) < 10:\n",
    "#             continue\n",
    "\n",
    "#         prev_len = -1\n",
    "#         while prev_len != len(leg['time']):\n",
    "#             prev_len = len(leg['time'])\n",
    "#             grad = np.gradient(leg['along'])\n",
    "#             keep_mask = grad > 0 if name == \"out\" else grad < 0\n",
    "#             leg = leg.sel(time=keep_mask)\n",
    "\n",
    "#         if np.count_nonzero(~np.isnan(leg['along'])) < 100:\n",
    "#             continue\n",
    "\n",
    "#         results[name] = interpolate(leg.set_coords('along'))\n",
    "\n",
    "#     return results\n",
    "\n",
    "# def save_cleaned_transects(input_dir, output_dir):\n",
    "#     input_dir = Path(input_dir).expanduser()\n",
    "#     output_dir = Path(output_dir).expanduser()\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     for i, f in enumerate(sorted(input_dir.glob(\"*_grid_delayed.nc\"))):\n",
    "#         print(f\"🧼 Cleaning & interpolating: {f.name}\")\n",
    "#         try:\n",
    "#             results = clean_and_interpolate(str(f))\n",
    "\n",
    "#             for leg in results:\n",
    "#                 ds = results[leg]\n",
    "#                 ds = ds.expand_dims('transect')\n",
    "#                 ds = ds.assign_coords(transect=(\"transect\", [i]))\n",
    "\n",
    "#                 out_path = output_dir / f\"{f.stem}_{leg}.nc\"\n",
    "#                 ds.to_netcdf(out_path)\n",
    "#                 print(f\"✅ Saved {leg}: {out_path.name}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Failed on {f.name}: {e}\")\n",
    "if False:\n",
    "    save_cleaned_transects(\n",
    "        input_dir=\"~/CalvertLine_reprocessed\",\n",
    "        output_dir=\"~/Desktop/cleaned_transects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xarray as xr\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import re\n",
    "# from pathlib import Path\n",
    "\n",
    "# # === Directories ===\n",
    "# input_dir = Path(\"~/Desktop/cleaned_transects\").expanduser()\n",
    "# output_dir = Path(\"~/Desktop/CalvertLine_cubes\").expanduser()\n",
    "# # output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # === Extract year from filename ===\n",
    "# def extract_year(filename):\n",
    "#     match = re.search(r'(\\d{8})', filename)\n",
    "#     return match.group(1)[:4] if match else None\n",
    "\n",
    "# # === Interpolation and cube appending logic (unchanged) ===\n",
    "# def append_to_calvert_cube(ds_transect, cube_path, source_file=None):\n",
    "#     cube_path = os.path.expanduser(cube_path)\n",
    "\n",
    "#     if 'transect' not in ds_transect.dims:\n",
    "#         ds_transect = ds_transect.expand_dims(transect=[0])\n",
    "\n",
    "#     if 'transect_time' not in ds_transect.coords:\n",
    "#         valid_times = pd.to_datetime(ds_transect['time'].values, errors='coerce').dropna()\n",
    "#         transect_time = np.datetime64(valid_times[0]) if len(valid_times) > 0 else np.datetime64('NaT')\n",
    "#         ds_transect = ds_transect.assign_coords(transect_time=('transect', [transect_time]))\n",
    "\n",
    "#     if source_file:\n",
    "#         ds_transect.attrs['source_file'] = os.path.basename(source_file)\n",
    "\n",
    "#     if not os.path.exists(cube_path):\n",
    "#         ds_transect.to_netcdf(cube_path)\n",
    "#         print(f\"🟢 Created new cube: {cube_path}\")\n",
    "#         return\n",
    "\n",
    "#     cube = xr.open_dataset(cube_path)\n",
    "#     cube.load(); cube.close()\n",
    "\n",
    "#     processed_files = cube.attrs.get('source_file', [])\n",
    "#     if isinstance(processed_files, str):\n",
    "#         processed_files = [processed_files]\n",
    "#     if os.path.basename(source_file) in processed_files:\n",
    "#         print(f\"⚠️ Skipping {source_file}: already in cube.\")\n",
    "#         return\n",
    "\n",
    "#     new_index = cube.sizes['transect']\n",
    "#     ds_transect = ds_transect.assign_coords(transect=[new_index])\n",
    "#     ds_combined = xr.concat([cube, ds_transect], dim='transect')\n",
    "#     ds_combined = ds_combined.sortby('transect_time')\n",
    "\n",
    "#     ds_combined.attrs['source_file'] = processed_files + [os.path.basename(source_file)]\n",
    "#     ds_combined.to_netcdf(cube_path, mode='w')\n",
    "#     print(f\"✅ Appended: {source_file}\")\n",
    "\n",
    "# if False:\n",
    "#     # === Main loop: group by year and build each cube ===\n",
    "#     all_nc_files = sorted(input_dir.glob(\"*.nc\"))\n",
    "\n",
    "#     files_by_year = {}\n",
    "#     for f in all_nc_files:\n",
    "#         year = extract_year(f.name)\n",
    "#         if year:\n",
    "#             files_by_year.setdefault(year, []).append(f)\n",
    "\n",
    "#     for year, files in sorted(files_by_year.items()):\n",
    "#         print(f\"\\n📅 Processing year {year} with {len(files)} files\")\n",
    "#         cube_path = str(output_dir / f\"cube_{year}.nc\")\n",
    "#         for f in files:\n",
    "#             try:\n",
    "#                 ds = xr.open_dataset(f)\n",
    "#                 append_to_calvert_cube(ds, cube_path=cube_path, source_file=str(f))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"❌ Failed to process {f.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cube_2025 = xr.open_dataset('/Users/martinwilliamson/Desktop/CalvertLine_cubes/cube_2025.nc')\n",
    "# cube_2025\n",
    "\n",
    "# cube_2024 = xr.open_dataset('/Users/martinwilliamson/Desktop/CalvertLine_cubes/cube_2024.nc')\n",
    "# cube_2024\n",
    "\n",
    "# transect_times = cube_2024['transect_time'].values\n",
    "# transect_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cube_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851acc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# import re\n",
    "\n",
    "# urls = [\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-bb046/dfo-bb046-20210511/L0-gridfiles/dfo-bb046-20210511_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-bb046/dfo-bb046-20220507/L0-gridfiles/dfo-bb046-20220507_grid.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-bb046/dfo-bb046-20210413/L0-gridfiles/dfo-bb046-20210413_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-bb046/dfo-bb046-20200717/L0-gridfiles/dfo-bb046-20200717_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-bb046/dfo-bb046-20201006/L0-gridfiles/dfo-bb046-20201006_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-bb046/dfo-bb046-20210212/L0-gridfiles/dfo-bb046-20210212_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-bb046/dfo-bb046-20220707/L0-gridfiles/dfo-bb046-20220707_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-bb046/dfo-bb046-20201103/L0-gridfiles/dfo-bb046-20201103_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-bb046/dfo-bb046-20200810/L0-gridfiles/dfo-bb046-20200810_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-bb046/dfo-bb046-20210324/L0-gridfiles/dfo-bb046-20210324_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-bb046/dfo-bb046-20200908/L0-gridfiles/dfo-bb046-20200908_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-bb046/dfo-bb046-20220608/L0-gridfiles/dfo-bb046-20220608_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-k999/dfo-k999-20230320/L0-gridfiles/dfo-k999-20230320_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-k999/dfo-k999-20230811/L0-gridfiles/dfo-k999-20230811_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-k999/dfo-k999-20250114/L0-gridfiles/dfo-k999-20250114_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-k999/dfo-k999-20241023/L0-gridfiles/dfo-k999-20241023_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-k999/dfo-k999-20230915/L0-gridfiles/dfo-k999-20230915_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-k999/dfo-k999-20250317/L0-gridfiles/dfo-k999-20250317_grid.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-k999/dfo-k999-20241119/L0-gridfiles/dfo-k999-20241119_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-k999/dfo-k999-20230516/L0-gridfiles/dfo-k999-20230516_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-k999/dfo-k999-20230418/L0-gridfiles/dfo-k999-20230418_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-hal1002/dfo-hal1002-20220914/L0-gridfiles/dfo-hal1002-20220914_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-hal1002/dfo-hal1002-20220804/L0-gridfiles/dfo-hal1002-20220804_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-hal1002/dfo-hal1002-20240723/L0-gridfiles/dfo-hal1002-20240723_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-hal1002/dfo-hal1002-20250311/L0-gridfiles/dfo-hal1002-20250311_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-hal1002/dfo-hal1002-20240702/L0-gridfiles/dfo-hal1002-20240702_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-hal1002/dfo-hal1002-20240924/L0-gridfiles/dfo-hal1002-20240924_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-hal1002/dfo-hal1002-20250506/L0-gridfiles/dfo-hal1002-20250506_grid.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-eva035/dfo-eva035-20190612/L0-gridfiles/dfo-eva035-20190612_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-eva035/dfo-eva035-20230915/L0-gridfiles/dfo-eva035-20230915_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-eva035/dfo-eva035-20230811/L0-gridfiles/dfo-eva035-20230811_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-eva035/dfo-eva035-20230720/L0-gridfiles/dfo-eva035-20230720_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-eva035/dfo-eva035-20230620/L0-gridfiles/dfo-eva035-20230620_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-eva035/dfo-eva035-20230518/L0-gridfiles/dfo-eva035-20230518_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-eva035/dfo-eva035-20231019/L0-gridfiles/dfo-eva035-20231019_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-mike579/dfo-mike579-20190611/L0-gridfiles/dfo-mike579-20190611_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-mike579/dfo-mike579-20210704/L0-gridfiles/dfo-mike579-20210704_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-colin1142/dfo-colin1142-20240312/L0-gridfiles/dfo-colin1142-20240312_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-marvin1003/dfo-marvin1003-20240416/L0-gridfiles/dfo-marvin1003-20240416_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-marvin1003/dfo-marvin1003-20221129/L0-gridfiles/dfo-marvin1003-20221129_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-marvin1003/dfo-marvin1003-20240516/L0-gridfiles/dfo-marvin1003-20240516_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-marvin1003/dfo-marvin1003-20221018/L0-gridfiles/dfo-marvin1003-20221018_grid_delayed.nc\",\n",
    "#     \"https://cproof.uvic.ca/gliderdata/deployments/./dfo-rosie713/dfo-rosie713-20190615/L0-gridfiles/dfo-rosie713-20190615_grid_delayed.nc\"\n",
    "# ]\n",
    "\n",
    "# import re\n",
    "# from collections import defaultdict\n",
    "\n",
    "# by_year = defaultdict(list)\n",
    "# for url in urls:\n",
    "#     filename = url.split('/')[-1]\n",
    "#     match = re.search(r'(\\d{4})(\\d{4})', filename)\n",
    "#     if match:\n",
    "#         year = match.group(1)\n",
    "#         full_date = match.group(1) + match.group(2)\n",
    "#         by_year[year].append(full_date)\n",
    "\n",
    "# # Sort years and timestamps within each year\n",
    "# sorted_by_year = {year: sorted(timestamps) for year, timestamps in sorted(by_year.items())}\n",
    "# sorted_by_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dabf943",
   "metadata": {},
   "source": [
    "# Here I am attempting to figure out why some missions didn't get plotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e6cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.dates as mdates\n",
    "import cmocean as cm\n",
    "import waypoint_distance as wd\n",
    "import pandas as pd\n",
    "\n",
    "def plot_all_sections(cube, xlim = 77):\n",
    "    \"\"\"\n",
    "    Plot temperature sections for all transects in the data cube.\n",
    "\n",
    "    Parameters:\n",
    "    - cube: xarray.Dataset, the combined data cube of all transects.\n",
    "    - temp_bounds: tuple, (min, max) temperature values for colormap.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 12,\n",
    "        'axes.titlesize': 20,\n",
    "        'axes.labelsize': 20,\n",
    "        'xtick.labelsize': 15,\n",
    "        'ytick.labelsize': 15,\n",
    "        'legend.fontsize': 20,\n",
    "        'figure.titlesize': 20})\n",
    "    \n",
    "    topo_file = os.path.expanduser('~/Desktop/british_columbia_3_msl_2013.nc')\n",
    "    topo = xr.open_dataset(topo_file)\n",
    "\n",
    "    for transect in cube.transect:\n",
    "        ds = cube.sel(transect=transect)\n",
    "\n",
    "        temp_bounds = (5.3, 10)\n",
    "        # Extract coordinates and data\n",
    "        along = ds['along'].values\n",
    "        depth = ds['depth'].values\n",
    "        temperature = ds['temperature'].values\n",
    "        pdens = ds['potential_density'].values - 1000  # Sigma-theta\n",
    "\n",
    "        # Interpolate bathymetry over full along track\n",
    "        full_lon = ds['longitude'].values\n",
    "        full_lat = ds['latitude'].values\n",
    "\n",
    "        interp_bathy = topo['Band1'].interp(\n",
    "            lon=xr.DataArray(full_lon, dims='along'),\n",
    "            lat=xr.DataArray(full_lat, dims='along'),\n",
    "            method='nearest')\n",
    "        bottom_depths = -interp_bathy.values  # 1D array for full along\n",
    "\n",
    "        # Plot temperature section\n",
    "        fig, ax = plt.subplots(figsize=( 1.5 * 1.5 * 6.4, 1.5 * 4.8))\n",
    "        # Create meshgrid of depth and along\n",
    "        depth_grid, along_grid = np.meshgrid(depth, along, indexing='ij')\n",
    "\n",
    "        # Expand bottom_depths to match depth grid shape\n",
    "        bathymetry_floor = np.tile(bottom_depths, (len(depth), 1))\n",
    "\n",
    "        # Create mask where depth > bottom\n",
    "        mask = depth_grid > bathymetry_floor\n",
    "\n",
    "        # Plot grey background\n",
    "        ax.contourf(along / 1000, depth, mask,\n",
    "                    levels=[0.5, 1.5], colors='grey', zorder=1)\n",
    "        cf = ax.pcolormesh(along / 1000, depth, temperature, shading='auto',\n",
    "                           cmap=cm.cm.thermal, vmin=temp_bounds[0], vmax=temp_bounds[1])\n",
    "\n",
    "        # Bathymetry\n",
    "        ax.plot(along / 1000, bottom_depths, color='black', linewidth=2)\n",
    "\n",
    "        # Isopycnal contours\n",
    "        for levels, color, lw in [\n",
    "            (np.linspace(24, 27, 7), 'black', 0.5),\n",
    "            ([26.6], 'white', 2),\n",
    "            ([26.7], 'lime', 2),\n",
    "            ([26.8], 'red', 2),\n",
    "            ([26.9], 'blue', 2)]:\n",
    "            cf_iso = ax.contour(along / 1000, depth, pdens, levels=levels,\n",
    "                                colors=color, linewidths=lw, linestyles='-')\n",
    "            if lw != 0.3:\n",
    "                ax.clabel(cf_iso, fmt='%1.2f')\n",
    "\n",
    "        time_top = ds['time'].values  # shape (along,)\n",
    "        along_km = along / 1000\n",
    "\n",
    "        # Number of ticks you want\n",
    "        nticks = 8\n",
    "        idx_ticks = np.linspace(0, len(along_km) - 1, nticks, dtype=int)\n",
    "\n",
    "        tick_locs = along_km[idx_ticks]\n",
    "        tick_times = time_top[idx_ticks]\n",
    "\n",
    "        # Remove NaT values\n",
    "        valid_mask = ~pd.isna(tick_times)\n",
    "        tick_locs = tick_locs[valid_mask]\n",
    "        tick_times = tick_times[valid_mask]\n",
    "\n",
    "        # Format with hour and date\n",
    "        tick_labels = [pd.to_datetime(t).strftime('%b %d %H:%M') for t in tick_times]\n",
    "\n",
    "        # Add secondary x-axis with formatted time ticks\n",
    "        ax_top = ax.secondary_xaxis('top')\n",
    "        ax_top.set_xticks(tick_locs)\n",
    "        ax_top.set_xticklabels(tick_labels, rotation=30, ha='center', fontsize=10)\n",
    "\n",
    "        # Labels and formatting\n",
    "        ax.set_xlabel('Along-Transect Distance (km)')\n",
    "        ax.set_ylabel('Depth (m)')\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_ylim(410, 0)\n",
    "        ax.set_xlim(xlim,0)\n",
    "        tstr = str(ds['transect_time'].values.astype('datetime64[D]'))\n",
    "        ax.set_title(f'Temperature Section ({tstr})')\n",
    "        plt.colorbar(cf, ax=ax, label='Temperature (°C)')\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e920882",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = xr.open_dataset('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386dd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRYING AGAIN\" \n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import waypoint_distance as wd\n",
    "\n",
    "def interpolate(ds, step=50):\n",
    "    _, index_unique = np.unique(ds['along'], return_index=True)\n",
    "    ds = ds.isel(time=index_unique)\n",
    "\n",
    "    min_along = np.floor(ds['along'].min().item() / step) * step\n",
    "    max_along = np.ceil(ds['along'].max().item() / step) * step\n",
    "    along_grid = np.arange(min_along, max_along + 1, step)\n",
    "\n",
    "    ds = ds.swap_dims({'time': 'along'})\n",
    "    ds_interp = ds.interp(along=along_grid)\n",
    "\n",
    "    interp_time = np.interp(\n",
    "        along_grid,\n",
    "        ds['along'].values,\n",
    "        ds['time'].values.astype('datetime64[ns]').astype('float64'),\n",
    "        left=np.nan,\n",
    "        right=np.nan\n",
    "    )\n",
    "    ds_interp['time'] = ('along', interp_time.astype('datetime64[ns]'))\n",
    "\n",
    "    return ds_interp\n",
    "\n",
    "# def clean_and_interpolate(file_pathway):\n",
    "#     ds = xr.open_dataset('~/CalvertLine_reprocessed/dfo-colin1142-20240312_grid_delayed.nc')\n",
    "\n",
    "#     waypoint_lon = np.array([-127.950, -128.115, -128.243, -128.514, -128.646, -128.798])\n",
    "#     waypoint_lat = np.array([51.757, 51.705, 51.715, 51.450, 51.4165, 51.408])\n",
    "#     central_lat = 51.715\n",
    "\n",
    "#     alongx, acrossx, _ = wd.get_simple_distance(\n",
    "#         shiplon=ds['longitude'].values,\n",
    "#         shiplat=ds['latitude'].values,\n",
    "#         wplon=waypoint_lon,\n",
    "#         wplat=waypoint_lat,\n",
    "#         central_lat=central_lat)\n",
    "\n",
    "#     ds = ds.assign(along=('time', alongx), across=('time', acrossx))\n",
    "\n",
    "#     peak_idx = int(np.argmax(ds['along'].values))\n",
    "#     ds_out = ds.isel(time=slice(0, peak_idx + 1))\n",
    "#     ds_return = ds.isel(time=slice(peak_idx + 1, None))\n",
    "\n",
    "#     grad = np.gradient(ds_out['along'].values)\n",
    "#     grad\n",
    "#     results = {}\n",
    "#     for leg, name in [(ds_out, \"out\"), (ds_return, \"return\")]:\n",
    "#         if leg['time'].size < 2 or np.count_nonzero(~np.isnan(leg['along'])) < 10:\n",
    "#             continue\n",
    "\n",
    "#         prev_len = -1\n",
    "#         while prev_len != len(leg['time']):\n",
    "#             # if len(leg['along']) < 3:\n",
    "#             #     break  # not enough for gradient\n",
    "#             prev_len = len(leg['time'])\n",
    "#             grad = np.gradient(leg['along'])\n",
    "#             keep_mask = grad > 0 if name == \"out\" else grad < 0\n",
    "#             leg = leg.sel(time=keep_mask)\n",
    "\n",
    "#         if len(leg['along']) < 3 or np.count_nonzero(~np.isnan(leg['along'])) < 10:\n",
    "#             continue\n",
    "\n",
    "#         results[name] = interpolate(leg.set_coords('along'))\n",
    "\n",
    "#         return results\n",
    "    \n",
    "def clean_and_interpolate(file_pathway):\n",
    "    ds = xr.open_dataset(file_pathway)\n",
    "\n",
    "    waypoint_lon = np.array([-127.950, -128.115, -128.243, -128.514, -128.646, -128.798])\n",
    "    waypoint_lat = np.array([51.757, 51.705, 51.715, 51.450, 51.4165, 51.408])\n",
    "    central_lat = 51.715\n",
    "\n",
    "    alongx, acrossx, _ = wd.get_simple_distance(\n",
    "        shiplon=ds['longitude'].values,\n",
    "        shiplat=ds['latitude'].values,\n",
    "        wplon=waypoint_lon,\n",
    "        wplat=waypoint_lat,\n",
    "        central_lat=central_lat)\n",
    "\n",
    "    ds = ds.assign(along=('time', alongx), across=('time', acrossx))\n",
    "\n",
    "    peak_idx = int(np.argmax(ds['along'].values))\n",
    "    ds_out = ds.isel(time=slice(0, peak_idx + 1))\n",
    "    ds_return = ds.isel(time=slice(peak_idx + 1, None))\n",
    "\n",
    "    results = {}\n",
    "    for leg, name in [(ds_out, \"out\"), (ds_return, \"return\")]:\n",
    "        if leg['time'].size < 2 or np.count_nonzero(~np.isnan(leg['along'])) < 10:\n",
    "            continue\n",
    "\n",
    "        prev_len = -1\n",
    "        while prev_len != len(leg['time']):\n",
    "            prev_len = len(leg['time'])\n",
    "            grad = np.gradient(leg['along'])\n",
    "            keep_mask = grad > 0 if name == \"out\" else grad < 0\n",
    "            leg = leg.sel(time=keep_mask)\n",
    "\n",
    "        if np.count_nonzero(~np.isnan(leg['along'])) < 100:\n",
    "            continue\n",
    "\n",
    "        results[name] = interpolate(leg.set_coords('along'))\n",
    "\n",
    "    return results\n",
    "\n",
    "# def save_cleaned_transects(input_dir, output_dir):\n",
    "#     input_dir = Path(input_dir).expanduser()\n",
    "#     output_dir = Path(output_dir).expanduser()\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     for i, f in enumerate(sorted(input_dir.glob(\"*_grid_delayed.nc\"))):\n",
    "#         print(f\"🧼 Cleaning & interpolating: {f.name}\")\n",
    "#         try:\n",
    "#             results = clean_and_interpolate(str(f))\n",
    "\n",
    "#             for leg in results:\n",
    "#                 ds = results[leg]\n",
    "#                 ds = ds.expand_dims('transect')\n",
    "#                 ds = ds.assign_coords(transect=(\"transect\", [i]))\n",
    "\n",
    "#                 out_path = output_dir / f\"{f.stem}_{leg}.nc\"\n",
    "#                 ds.to_netcdf(out_path)\n",
    "#                 print(f\"✅ Saved {leg}: {out_path.name}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Failed on {f.name}: {e}\")\n",
    "# if False:\n",
    "#     save_cleaned_transects(\n",
    "#         input_dir=\"~/CalvertLine_reprocessed\",\n",
    "#         output_dir=\"~/Desktop/cleaned_transects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6021847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import xarray as xr\n",
    "\n",
    "# # ⬇️ Step 1: Point to files you want to process\n",
    "# files_to_run = [\n",
    "#     \"~/CalvertLine_reprocessed/dfo-eva035-20231019_grid_delayed.nc\",\n",
    "#     \"~/CalvertLine_reprocessed/dfo-hal1002-20240924_grid_delayed.nc\",\n",
    "#     \"~/CalvertLine_reprocessed/dfo-marvin1003-20221129_grid_delayed.nc\",\n",
    "#     \"~/CalvertLine_reprocessed/dfo-marvin1003-20240516_grid_delayed.nc\"\n",
    "# ]\n",
    "\n",
    "# # ⬇️ Step 2: Destination for cleaned outbound files\n",
    "# output_dir = Path(\"~/Desktop/cleaned_transects\").expanduser()\n",
    "# output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # ⬇️ Step 3: Loop through files and run your function\n",
    "# for i, file_path in enumerate(files_to_run):\n",
    "#     file_path = Path(file_path).expanduser()\n",
    "#     print(f\"🧼 Cleaning outbound: {file_path.name}\")\n",
    "    \n",
    "#     try:\n",
    "#         results = clean_and_interpolate(str(file_path))\n",
    "\n",
    "#         if \"out\" in results:\n",
    "#             ds = results[\"out\"]\n",
    "#             ds = ds.expand_dims(\"transect\")\n",
    "#             ds = ds.assign_coords(transect=(\"transect\", [i]))\n",
    "\n",
    "#             out_path = output_dir / f\"{file_path.stem}_out.nc\"\n",
    "#             ds.to_netcdf(out_path)\n",
    "#             print(f\"✅ Saved outbound: {out_path.name}\")\n",
    "#         else:\n",
    "#             print(\"⚠️ No outbound leg found.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Failed on {file_path.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a99b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_missions_map(glider_files, long_bounds=None, lat_bounds=None,\n",
    "                               topo_file=os.path.expanduser('~/Desktop/british_columbia_3_msl_2013.nc')):\n",
    "    \"\"\"\n",
    "    Plot multiple glider paths over bathymetry for the Calvert Line.\n",
    "\n",
    "    Parameters:\n",
    "    - glider_files: list of str, paths to NetCDF glider grid files\n",
    "    - topo_file: str, path to topo NetCDF file\n",
    "    - long_bounds, lat_bounds: optional map bounds\n",
    "    \"\"\"\n",
    "\n",
    "    # Load all datasets\n",
    "    datasets = [xr.open_dataset(os.path.expanduser(f)) for f in glider_files]\n",
    "    all_lons = np.concatenate([ds['longitude'].values for ds in datasets])\n",
    "    all_lats = np.concatenate([ds['latitude'].values for ds in datasets])\n",
    "\n",
    "    # Auto bounding box if not provided\n",
    "    if long_bounds is None:\n",
    "        long_bounds = [all_lons.min() - 0.5, all_lons.max() + 0.5]\n",
    "    if lat_bounds is None:\n",
    "        lat_bounds = [all_lats.min() - 0.5, all_lats.max() + 0.5]\n",
    "\n",
    "    # Load topo and subset\n",
    "    topo = xr.open_dataset(topo_file)\n",
    "    topo = topo.sel(\n",
    "        lon=slice(long_bounds[0], long_bounds[1]),\n",
    "        lat=slice(lat_bounds[0], lat_bounds[1])\n",
    "    )\n",
    "    topo_var = -topo['Band1']\n",
    "\n",
    "    # Set up plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 9), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.set_extent(long_bounds + lat_bounds, crs=ccrs.PlateCarree())\n",
    "\n",
    "    # Gridlines with lat/lon ticks only\n",
    "    gl = ax.gridlines(draw_labels=True, linestyle='--', alpha=0)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "\n",
    "    # Bathymetry shading\n",
    "    levels = np.linspace(0, 410, 51)\n",
    "    contourf = ax.contourf(topo['lon'], topo['lat'], topo_var,\n",
    "                           levels=levels, cmap=cm.cm.deep, extend='both')\n",
    "    fig.colorbar(contourf, ax=ax, label='Depth (m)')\n",
    "\n",
    "    # 0 m contour (coastline)\n",
    "    ax.contour(topo['lon'], topo['lat'], topo_var, levels=[0.5], colors='black', linewidths=1)\n",
    "\n",
    "    # Time scaling for consistent colorbar\n",
    "    all_time_vals = np.concatenate([ds['time'].values for ds in datasets])\n",
    "    all_time_nums = mdates.date2num(all_time_vals)\n",
    "    vmin = all_time_nums.min()\n",
    "    vmax = all_time_nums.max()\n",
    "\n",
    "    for i, ds in enumerate(datasets):\n",
    "        lons = ds['longitude'].values\n",
    "        lats = ds['latitude'].values\n",
    "        time_vals = ds['time'].values\n",
    "        time_nums = mdates.date2num(time_vals)\n",
    "\n",
    "        label = os.path.basename(glider_files[i]).split('_')[0]\n",
    "        sc = ax.scatter(lons, lats, c=time_nums, cmap='seismic',\n",
    "                        vmin=vmin, vmax=vmax, s=5, transform=ccrs.PlateCarree(),\n",
    "                        zorder=5, label=label)\n",
    "\n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(sc, ax=ax, orientation='vertical', pad=0.01, extend='both')\n",
    "    cbar.set_label('Date')\n",
    "    cbar.ax.yaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "    # Waypoint track overlay (optional)\n",
    "    waypoint_lon = [-127.950, -128.115, -128.243, -128.514, -128.646, -128.798]\n",
    "    waypoint_lat = [51.757, 51.705, 51.715, 51.450, 51.4165, 51.408]\n",
    "    # ax.plot(waypoint_lon, waypoint_lat, color='black', linestyle='-', linewidth=2, label='Transect')\n",
    "\n",
    "    ax.legend(title='Glider Missions')\n",
    "    ax.set_title('Glider Missions Map')\n",
    "    ax.set_aspect(1 / np.cos(np.deg2rad(np.mean(lat_bounds))))\n",
    "\n",
    "plot_multiple_missions_map(['~/CalvertLine_reprocessed/dfo-colin1142-20240312_grid_delayed.nc'], long_bounds=(-128.3, -128), lat_bounds = (51.6, 51.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e170297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def clean_and_interpolate(file_pathway):\n",
    "    ds = xr.open_dataset(file_pathway)\n",
    "\n",
    "    waypoint_lon = np.array([-127.950, -128.115, -128.243, -128.514, -128.646, -128.798])\n",
    "    waypoint_lat = np.array([51.757, 51.705, 51.715, 51.450, 51.4165, 51.408])\n",
    "    central_lat = 51.715\n",
    "\n",
    "    alongx, acrossx, _ = wd.get_simple_distance(\n",
    "        shiplon=ds['longitude'].values,\n",
    "        shiplat=ds['latitude'].values,\n",
    "        wplon=waypoint_lon,\n",
    "        wplat=waypoint_lat,\n",
    "        central_lat=central_lat)\n",
    "\n",
    "    ds = ds.assign(along=('time', alongx), across=('time', acrossx))\n",
    "\n",
    "    peak_idx = int(np.argmax(ds['along'].values))\n",
    "    ds_out = ds.isel(time=slice(0, peak_idx + 1))\n",
    "    ds_return = ds.isel(time=slice(peak_idx + 1, None))\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for leg, name in [(ds_out, \"out\"), (ds_return, \"return\")]:\n",
    "        try:\n",
    "            if leg['time'].size < 2 or np.count_nonzero(~np.isnan(leg['along'])) < 10:\n",
    "                continue\n",
    "\n",
    "            prev_len = -1\n",
    "            while prev_len != len(leg['time']):\n",
    "                if len(leg['along']) < 3:\n",
    "                    raise ValueError(\"Too short for gradient\")\n",
    "                prev_len = len(leg['time'])\n",
    "                grad = np.gradient(leg['along'].values)\n",
    "                keep_mask = grad > 0 if name == \"out\" else grad < 0\n",
    "                leg = leg.sel(time=keep_mask)\n",
    "\n",
    "            if np.count_nonzero(~np.isnan(leg['along'])) < 100:\n",
    "                continue\n",
    "\n",
    "            results[name] = interpolate(leg.set_coords('along'))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {name} leg in {file_pathway}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return results\n",
    "input_dir = Path(\"~/CalvertLine_reprocessed\").expanduser()\n",
    "output_dir = Path(\"~/Desktop/cleaned_transects\").expanduser()\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_files = sorted(input_dir.glob(\"*_grid_delayed.nc\"))\n",
    "\n",
    "for file_path in all_files:\n",
    "    print(f\"🧼 Cleaning & interpolating: {file_path.name}\")\n",
    "    \n",
    "    try:\n",
    "        results = clean_and_interpolate(str(file_path))\n",
    "\n",
    "        for leg in (\"out\", \"return\"):\n",
    "            if leg in results:\n",
    "                ds = results[leg]\n",
    "                ds = ds.expand_dims(\"transect\")\n",
    "\n",
    "                # Get first valid time as date string\n",
    "                valid_times = ds['time'].values[~np.isnan(ds['time'].values)]\n",
    "                if len(valid_times) == 0:\n",
    "                    print(f\"⚠️ Skipping {file_path.name} — no valid times in {leg} leg.\")\n",
    "                    continue\n",
    "                timestamp = pd.to_datetime(valid_times[0]).strftime('%Y%m%d')\n",
    "\n",
    "                # Assign coordinate and build filename\n",
    "                ds = ds.assign_coords(transect=(\"transect\", [timestamp]))\n",
    "                out_path = output_dir / f\"{timestamp}_{leg}.nc\"\n",
    "                \n",
    "                ds.to_netcdf(out_path)\n",
    "                print(f\"✅ Saved {leg}: {out_path.name}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No {leg} leg found in {file_path.name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed on {file_path.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab65db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_plot(ds, var='temperature'):\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ds[var].plot(ax=ax)\n",
    "    ax.set_title(f\"{var} — {ds.attrs.get('title', '')}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e952b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bad_data(ds, file_id):\n",
    "    if file_id == '20200717':\n",
    "        ds = ds.where(ds.salinity > 26, drop=True)\n",
    "    elif file_id == '20200810':\n",
    "        ds = ds.sel(time=~ds.time.isin([\n",
    "            np.datetime64(\"2020-08-10T04:33:00\"),\n",
    "            np.datetime64(\"2020-08-10T04:34:00\")\n",
    "        ]))\n",
    "    return ds\n",
    "file_id = file_path.name.split('-')[-1].split('_')[0]  # e.g., '20200810'\n",
    "ds = clean_bad_data(ds, file_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beecc8c",
   "metadata": {},
   "source": [
    "# August 6th fixing some deep casts and changing masking logic slightly #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b122e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.dates as mdates\n",
    "import cmocean as cm\n",
    "import waypoint_distance as wd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# def plot_section(ds, topo, xlim=(77,0)):\n",
    "#     \"\"\"\n",
    "#     Plot temperature section for a given file\n",
    "\n",
    "#     Parameters:\n",
    "#     - ds: xarray.Dataset, a .nc file for each transect\n",
    "#     - xlim: float, maximum distance (km) along transect to plot\n",
    "#     \"\"\"\n",
    "\n",
    "#     # ─── Styling ─────────────────────────────────────────\n",
    "#     plt.rcParams.update({\n",
    "#         'font.size': 12,\n",
    "#         'axes.titlesize': 20,\n",
    "#         'axes.labelsize': 20,\n",
    "#         'xtick.labelsize': 15,\n",
    "#         'ytick.labelsize': 15,\n",
    "#         'legend.fontsize': 20,\n",
    "#         'figure.titlesize': 20})\n",
    "\n",
    "#     # ─── Extract variables ──────────────────────────────\n",
    "#     along = ds['along'].values\n",
    "#     depth = ds['depth'].values\n",
    "#     temperature = ds['temperature'].values\n",
    "#     pdens = ds['potential_density'].values - 1000  # sigma-theta\n",
    "#     lon = ds['longitude'].values\n",
    "#     lat = ds['latitude'].values\n",
    "#     time_top = ds['time'].values  # shape (along,)\n",
    "\n",
    "#     # ─── Interpolate Bathymetry ─────────────────────────\n",
    "#     interp_bathy = topo['Band1'].interp(\n",
    "#         lon=xr.DataArray(lon, dims='along'),\n",
    "#         lat=xr.DataArray(lat, dims='along'),\n",
    "#         method='nearest')\n",
    "#     ocean_floor = -interp_bathy.values\n",
    "#     depth_grid, along_grid = np.meshgrid(depth, along, indexing='ij')  # shape: (depth, along)\n",
    "\n",
    "#     # Expand bottom_depths to match depth grid shape\n",
    "#     bathymetry_floor = np.tile(ocean_floor, (len(depth), 1))\n",
    "\n",
    "#     # Create mask where depth > bottom\n",
    "#     mask = depth_grid > bathymetry_floor\n",
    "\n",
    "#     # ------- Plotting ------- #\n",
    "#     fig, ax = plt.subplots(figsize=(1.5 * 1.5 * 6.4, 1.5 * 4.8))\n",
    "    \n",
    "#     # Fill below bathymetry to 420 m\n",
    "#     ax.fill_between(along / 1000, ocean_floor, 420,\n",
    "#                     where=~np.isnan(ocean_floor),\n",
    "#                     facecolor='grey', zorder=1)\n",
    "    \n",
    "#     # Plot temperature\n",
    "#     cf = ax.pcolormesh(along / 1000, depth, temperature, \n",
    "#                     shading='auto', cmap=cm.cm.thermal, \n",
    "#                     vmin=5.3, vmax=10, zorder=2)\n",
    "\n",
    "#     # Plot bathymetry\n",
    "#     ax.plot(along / 1000, ocean_floor, color='black', linewidth=2)\n",
    "\n",
    "#     # Isopycnals\n",
    "#     for levels, color, lw in [\n",
    "#         (np.linspace(24, 27, 7), 'black', 0.5),\n",
    "#         # ([26.6], 'white', 2),\n",
    "#         ([26.7], 'lime', 2),\n",
    "#         ([26.8], 'red', 2),\n",
    "#         ([26.9], 'blue', 2)]:\n",
    "#         cf_iso = ax.contour(along / 1000, depth, pdens, levels=levels,\n",
    "#                             colors=color, linewidths=lw, linestyles='-')\n",
    "#         if lw != 0.3:\n",
    "#             ax.clabel(cf_iso, fmt='%1.2f')\n",
    "\n",
    "#     # ─── Top Axis with Time Labels ──────────────────────\n",
    "#     along_km = along / 1000\n",
    "#     nticks = 8\n",
    "#     idx_ticks = np.linspace(0, len(along_km) - 1, nticks, dtype=int)\n",
    "#     tick_locs = along_km[idx_ticks]\n",
    "#     tick_times = time_top[idx_ticks]\n",
    "\n",
    "#     # Remove NaT\n",
    "#     valid_mask = ~pd.isna(tick_times)\n",
    "#     tick_locs = tick_locs[valid_mask]\n",
    "#     tick_times = tick_times[valid_mask]\n",
    "#     tick_labels = [pd.to_datetime(t).strftime('%b %d %H:%M') for t in tick_times]\n",
    "\n",
    "#     ax_top = ax.secondary_xaxis('top')\n",
    "#     ax_top.set_xticks(tick_locs)\n",
    "#     ax_top.set_xticklabels(tick_labels, rotation=30, ha='center', fontsize=10)\n",
    "\n",
    "#     # ─── Labels and Limits ──────────────────────────────\n",
    "#     ax.set_xlabel('Along-Transect Distance (km)')\n",
    "#     ax.set_ylabel('Depth (m)')\n",
    "#     ax.invert_yaxis()\n",
    "#     ax.set_ylim(420, 0)\n",
    "#     ax.set_xlim(xlim)\n",
    "\n",
    "#     earliest_time = pd.to_datetime(min(tick_times))\n",
    "#     tstr = earliest_time.strftime('%Y-%m-%d')\n",
    "#     ax.set_title(f'Temperature Section ({tstr})')\n",
    "\n",
    "#     cf = ax.pcolormesh(along / 1000, depth, temperature, \n",
    "#                     shading='auto', cmap=cm.cm.thermal, \n",
    "#                    vmin=5.3, vmax=10, zorder=2)\n",
    "\n",
    "#     plt.colorbar(cf, ax=ax, label='Temperature (°C)')\n",
    "#     plt.tight_layout()\n",
    "\n",
    "def interpolate(ds, step=50, extrapolate=True):\n",
    "    import numpy as np\n",
    "    import xarray as xr\n",
    "\n",
    "    # Remove duplicate along values\n",
    "    _, index_unique = np.unique(ds['along'], return_index=True)\n",
    "    ds = ds.isel(time=index_unique)\n",
    "\n",
    "    # Build regular along grid\n",
    "    min_along = np.floor(ds['along'].min().item() / step) * step\n",
    "    max_along = np.ceil(ds['along'].max().item() / step) * step\n",
    "    along_grid = np.arange(min_along, max_along + 1, step)\n",
    "\n",
    "    # Swap time with along for interpolation\n",
    "    ds = ds.swap_dims({'time': 'along'})\n",
    "    ds_interp = ds.interp(along=along_grid)\n",
    "\n",
    "    # Interpolate time manually (since it's not numeric by default)\n",
    "    interp_time = np.interp(\n",
    "        along_grid,\n",
    "        ds['along'].values,\n",
    "        ds['time'].values.astype('datetime64[ns]').astype('float64'),\n",
    "        left=np.nan,\n",
    "        right=np.nan\n",
    "    )\n",
    "    ds_interp['time'] = ('along', interp_time.astype('datetime64[ns]'))\n",
    "\n",
    "    # Get along mask for 0–20 km region\n",
    "    mask_along = (ds_interp['along'] >= 0) & (ds_interp['along'] <= 20000)\n",
    "\n",
    "    # Subset temperature in that range\n",
    "    temp_sub = ds_interp['temperature'].where(mask_along, drop=True)\n",
    "\n",
    "    # Find the deepest depth where at least one valid (non-NaN) temperature exists\n",
    "    valid_depths = ds_interp['depth'][~np.all(np.isnan(temp_sub), axis=1)]\n",
    "    max_valid_depth = valid_depths.max().item()\n",
    "\n",
    "    # Limit to valid depth range\n",
    "    ds_interp = ds_interp.sel(depth=ds_interp['depth'] <= max_valid_depth)\n",
    "    # Drop any depths that aren't exactly on a 1.0 m spacing starting from 0.5\n",
    "    depths = ds_interp['depth'].values\n",
    "    uniform_depths = np.arange(0.5, np.max(depths) + 1, 1.0)\n",
    "    ds_interp = ds_interp.sel(depth=np.isin(ds_interp['depth'], uniform_depths))\n",
    "\n",
    "    # ─── Vectorized valid_temp_depth assignment ─────\n",
    "    temp = ds_interp['temperature'].values  # (depth, along)\n",
    "    depth_vals = ds_interp['depth'].values\n",
    "    valid_mask = ~np.isnan(temp)\n",
    "    reversed_mask = valid_mask[::-1, :]\n",
    "    first_valid_idx_from_bottom = reversed_mask.argmax(axis=0)\n",
    "    has_valid_data = valid_mask.any(axis=0)\n",
    "    valid_depths = np.where(has_valid_data,\n",
    "                            depth_vals[-1 - first_valid_idx_from_bottom],\n",
    "                            np.nan)\n",
    "    ds_interp['valid_temp_depth'] = ('along', valid_depths)\n",
    "\n",
    "    # ─── Fill NaNs along each depth row using nearest ─────\n",
    "    for var in ds_interp.data_vars:\n",
    "        da = ds_interp[var]\n",
    "        if 'along' not in da.dims or 'depth' not in da.dims:\n",
    "            continue\n",
    "\n",
    "        filled_rows = []\n",
    "        depths = ds_interp['depth'].values\n",
    "        along_vals = ds_interp['along'].values\n",
    "\n",
    "        for i in range(len(depths)):\n",
    "            row = da.isel(depth=i)\n",
    "            filled = row.interpolate_na(\n",
    "                dim='along',\n",
    "                method='nearest',\n",
    "                fill_value='extrapolate')\n",
    "            filled_rows.append(filled.values)\n",
    "\n",
    "        # Rebuild variable\n",
    "        new_da = xr.DataArray(\n",
    "            data=np.array(filled_rows),\n",
    "            dims=('depth', 'along'),\n",
    "            coords={'depth': depths, 'along': along_vals})\n",
    "        ds_interp[var] = new_da\n",
    "\n",
    "    return ds_interp\n",
    "\n",
    "def clean_and_interpolate(file_pathway, topo):\n",
    "\n",
    "    ds = xr.open_dataset(file_pathway)\n",
    "\n",
    "    waypoint_lon = np.array([-127.932, -128.013, -128.086415 , -128.154, -128.243, -128.345, -128.514, -128.646, -128.798])\n",
    "    waypoint_lat = np.array([51.775, 51.7415, 51.71175, 51.70317, 51.715, 51.70172, 51.450, 51.4165, 51.408])\n",
    "    central_lat = 51.715\n",
    "\n",
    "    alongx, acrossx, _ = wd.get_simple_distance(\n",
    "        shiplon=ds['longitude'].values,\n",
    "        shiplat=ds['latitude'].values,\n",
    "        wplon=waypoint_lon,\n",
    "        wplat=waypoint_lat,\n",
    "        central_lat=central_lat)\n",
    "\n",
    "    ds = ds.assign(along=('time', alongx), across=('time', acrossx))\n",
    "\n",
    "    peak_idx = int(np.argmax(ds['along'].values))\n",
    "    ds_out = ds.isel(time=slice(0, peak_idx + 1))\n",
    "    ds_return = ds.isel(time=slice(peak_idx + 1, None))\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for leg, name in [(ds_out, \"out\"), (ds_return, \"return\")]:\n",
    "        try:\n",
    "            if leg['time'].size < 2 or np.count_nonzero(~np.isnan(leg['along'])) < 10:\n",
    "                continue\n",
    "\n",
    "            # ─── Preserve deep casts (>300m) only if along < 20000 ───\n",
    "            depths = leg['depth'].values.reshape(-1, 1)\n",
    "            temps = leg['temperature'].values\n",
    "            along = leg['along'].values\n",
    "\n",
    "            # Find which casts go deeper than 300m\n",
    "            deep_cast_mask = np.nanmax(depths * ~np.isnan(temps), axis=0) > 300\n",
    "\n",
    "            # Only keep those casts if their along-location is < 20000\n",
    "            within_range_mask = along < 17000\n",
    "            final_mask = deep_cast_mask & within_range_mask\n",
    "\n",
    "            # Get corresponding times\n",
    "            preserve_times = leg['time'].values[final_mask]\n",
    "\n",
    "            # ─── Gradient filtering loop ───\n",
    "            prev_len = -1\n",
    "            while prev_len != len(leg['time']):\n",
    "                if len(leg['along']) < 3:\n",
    "                    raise ValueError(\"Too short for gradient\")\n",
    "                prev_len = len(leg['time'])\n",
    "                grad = np.gradient(leg['along'].values)\n",
    "                keep_mask = grad > 0 if name == \"out\" else grad < 0\n",
    "                leg = leg.sel(time=keep_mask)\n",
    "\n",
    "            # ─── Add back preserved deep profiles if dropped ───\n",
    "            times_to_add = [t for t in preserve_times if t not in leg['time'].values]\n",
    "            if times_to_add:\n",
    "                preserved = ds.sel(time=times_to_add)\n",
    "                leg = leg.sortby('along')\n",
    "                leg = xr.concat([leg, preserved], dim='time')\n",
    "\n",
    "            if np.count_nonzero(~np.isnan(leg['along'])) < 100:\n",
    "                continue\n",
    "\n",
    "            # Interpolate before assigning to results\n",
    "            leg_interp = interpolate(leg, step=50)\n",
    "            leg_interp = mask_dataset(leg_interp, topo)\n",
    "            results[name] = leg_interp\n",
    "            # results[name] = leg\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {name} leg in {file_pathway}: {e}\")\n",
    "            continue\n",
    "        \n",
    "    # Access outbound leg\n",
    "    ds_out_cleaned = results.get(\"out\")\n",
    "\n",
    "    # Access return leg\n",
    "    ds_return_cleaned = results.get(\"return\")\n",
    "\n",
    "    return ds_out_cleaned, ds_return_cleaned\n",
    "\n",
    "def mask_dataset(ds, topo):\n",
    "    \"\"\"\n",
    "    Applies depth-based masking to all 2D (depth, along) variables using valid_temp_depth\n",
    "    and bathymetry clearance logic.\n",
    "\n",
    "    Parameters:\n",
    "    - ds: xarray.Dataset, must include 'valid_temp_depth'\n",
    "    - topo: xarray.Dataset, bathymetry\n",
    "\n",
    "    Returns:\n",
    "    - ds_masked: xarray.Dataset with masked versions of all (depth, along) variables\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract variables\n",
    "    along = ds['along'].values\n",
    "    depth = ds['depth'].values\n",
    "    lon = ds['longitude'].values\n",
    "    lat = ds['latitude'].values\n",
    "\n",
    "    if 'valid_temp_depth' not in ds:\n",
    "        raise ValueError(\"Dataset must include 'valid_temp_depth'\")\n",
    "\n",
    "    # Interpolate bathymetry\n",
    "    ocean_floor = -topo['Band1'].interp(\n",
    "        lon=xr.DataArray(lon, dims='along'),\n",
    "        lat=xr.DataArray(lat, dims='along'),\n",
    "        method='nearest').values\n",
    "\n",
    "    valid_depths = ds['valid_temp_depth'].values\n",
    "    local_clearance = ocean_floor - valid_depths\n",
    "\n",
    "    # Average clearance in trusted region\n",
    "    trusted = (along >= 40000) & (along <= 77000)\n",
    "    mean_clearance = np.nanmean(local_clearance[trusted])\n",
    "\n",
    "    # Compute adaptive mask depth\n",
    "    mask_depth = np.minimum(\n",
    "        np.where(local_clearance <= mean_clearance,\n",
    "                 ocean_floor - local_clearance,\n",
    "                 ocean_floor - mean_clearance),\n",
    "        ocean_floor)\n",
    "\n",
    "    # ─── Extra masking of 5 m in deep basin (30–75 km) ───\n",
    "    along_km = along / 1000\n",
    "    extra_mask_zone = (along_km >= 40) & (along_km <= 75)\n",
    "    mask_depth[extra_mask_zone] = np.maximum(mask_depth[extra_mask_zone] - 5, 0)\n",
    "\n",
    "    # Apply mask to all 2D (depth, along) variables\n",
    "    for var in ds.data_vars:\n",
    "        da = ds[var]\n",
    "        if set(da.dims) == {'depth', 'along'}:\n",
    "            arr = da.values.copy()\n",
    "            for j in range(len(along)):\n",
    "                limit_depth = mask_depth[j]\n",
    "                if np.isnan(limit_depth):\n",
    "                    arr[:, j] = np.nan\n",
    "                else:\n",
    "                    arr[depth > limit_depth, j] = np.nan\n",
    "            # Save masked version\n",
    "            ds[var] = (('depth', 'along'), arr)\n",
    "            ds['mask_depth'] = ('along', mask_depth)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec01039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "file_path = Path(\"~/CalvertLine_reprocessed/dfo-hal1002-20240702_grid_delayed.nc\").expanduser()\n",
    "topo = xr.open_dataset(\"~/Desktop/Summer 2025 Python/british_columbia_3_msl_2013.nc\")\n",
    "\n",
    "ds_out, ds_return = clean_and_interpolate(str(file_path), topo)\n",
    "\n",
    "if ds_out is not None:\n",
    "    plot_section(ds_out, topo)\n",
    "else:\n",
    "    print(\"⚠️ No outbound leg found.\")\n",
    "\n",
    "if ds_return is not None:\n",
    "    plot_section(ds_return, topo)\n",
    "else:\n",
    "    print(\"⚠️ No outbound leg found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3ac91",
   "metadata": {},
   "source": [
    "For fun, I'll test some of the new data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecc76ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"~/Desktop/dfo-hal1002-20250701_grid.nc\").expanduser()\n",
    "topo = xr.open_dataset(\"~/Desktop/Summer 2025 Python/british_columbia_3_msl_2013.nc\")\n",
    "\n",
    "ds_out, ds_return = clean_and_interpolate(str(file_path), topo)\n",
    "\n",
    "if ds_out is not None:\n",
    "    plot_section(ds_out, topo)\n",
    "else:\n",
    "    print(\"⚠️ No outbound leg found.\")\n",
    "\n",
    "if ds_return is not None:\n",
    "    plot_section(ds_return, topo)\n",
    "else:\n",
    "    print(\"⚠️ No outbound leg found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c8c476",
   "metadata": {},
   "source": [
    "# The above reprocessing looks good, so I'm moving onto making a 2024-2025 cube now #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4109a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# ─── Directories ─────────────────────────────────────────\n",
    "input_dir = Path(\"~/CalvertLine_reprocessed\").expanduser()\n",
    "output_dir = Path(\"~/Desktop/Summer 2025 Python/cleaned_transects_2024_2025\").expanduser()\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ─── Load bathymetry once ───────────────────────────────\n",
    "topo = xr.open_dataset(\"~/Desktop/Summer 2025 Python/british_columbia_3_msl_2013.nc\")\n",
    "\n",
    "# ─── Gather files from 2024 & 2025 ───────────────────────\n",
    "all_files = sorted(input_dir.glob(\"*_grid*.nc\"))\n",
    "target_files = [f for f in all_files if any(y in f.name for y in [\"2024\", \"2025\"])]\n",
    "\n",
    "print(f\"🔍 Found {len(target_files)} files from 2024–2025\")\n",
    "\n",
    "# ─── Loop through files ──────────────────────────────────\n",
    "for file_path in target_files:\n",
    "    print(f\"\\n🧼 Cleaning: {file_path.name}\")\n",
    "\n",
    "    try:\n",
    "        ds_out, ds_return = clean_and_interpolate(str(file_path), topo)\n",
    "\n",
    "        for ds, leg in [(ds_out, \"out\"), (ds_return, \"return\")]:\n",
    "            if ds is None:\n",
    "                print(f\"⚠️ No {leg} leg in {file_path.name}\")\n",
    "                continue\n",
    "\n",
    "            valid_times = ds['time'].values[~np.isnan(ds['time'].values)]\n",
    "            if len(valid_times) == 0:\n",
    "                print(f\"⚠️ Skipping {file_path.name} — no valid times in {leg} leg.\")\n",
    "                continue\n",
    "\n",
    "            timestamp = pd.to_datetime(valid_times[0]).strftime('%Y%m%d')\n",
    "            out_path = output_dir / f\"{timestamp}_{leg}.nc\"\n",
    "            ds.to_netcdf(out_path)\n",
    "            print(f\"✅ Saved {leg}: {out_path.name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed on {file_path.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c07bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "input_dir = Path(\"~/Desktop/Summer 2025 Python/cleaned_transects_2024_2025\").expanduser()\n",
    "all_files = sorted(input_dir.glob(\"*.nc\"))\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for nc_file in all_files:\n",
    "    try:\n",
    "        ds = xr.open_dataset(nc_file)\n",
    "\n",
    "        # Promote lon/lat to regular variables if they are coordinates\n",
    "        for coord in ['longitude', 'latitude']:\n",
    "            if coord in ds.coords:\n",
    "                ds = ds.reset_coords(coord)\n",
    "\n",
    "        # Add transect label\n",
    "        transect_label = nc_file.stem\n",
    "        ds = ds.expand_dims(transect=[transect_label])\n",
    "        datasets.append(ds)\n",
    "\n",
    "        print(f\"✅ Added: {transect_label}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping {nc_file.name}: {e}\")\n",
    "\n",
    "# Combine all into one cube\n",
    "if datasets:\n",
    "    cube = xr.concat(datasets, dim='transect')\n",
    "    cube.to_netcdf(\"~/Desktop/2024_2025_transect_cube.nc\")\n",
    "    print(\"✅ Saved cube to ~/Desktop/2024_2025_transect_cube.nc\")\n",
    "else:\n",
    "    print(\"⚠️ No valid datasets to combine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736255f",
   "metadata": {},
   "source": [
    "That worked! signing off for now as of Wed Aug 6 @ 14:04\n",
    "\n",
    "edit: something was wrong (depths weren't all 1m spacing so plotting looked crazy weird, I fixed that and I also only kept deep casts within 20km of along cuz one of the dates did some weird stuff and this fixed it.) Made a new cube as of 15:08pm, and trying to plot again. Been working at this for 3 hours straight, wow processing data can be quite the adventure. \n",
    "\n",
    "Okay yahhh it worked. But why are some casts not showing up? cuzzz my 0km wasn't long enough.\n",
    "\n",
    "As of like 14:07, I've redefined my waypoints. Let's see if this new cube works. Gah now a data point that was being masked before for the weird stuff isnt being masked, changed the deep casts to be within the range of 17km now which is just within that new one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121eef0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2675e32",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
